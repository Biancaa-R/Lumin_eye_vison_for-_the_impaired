1.MAJOR:
Getting each frame as input and passing it as input to the model produces text respose but ..
 1.Output audio should not overlap
 2.Sampling period should be sufficiently large
 3.CONTINUOUS REPETITION OF SAME DATA SHOULD BE AVOIDED
 4.Should probably set some wait time.

2.MAJOR:
Including ocr with the general caption generation 
 + deploying them as two separate models and to get the output responses separately from them.
 - Integrating them into a single model... That checks if any text is available if not it proceeds with caption generation?
 //Harder method
 - BUT WILL USINNG THEM AS 2 SEPARATE TFLITE MODELS WORK IN RPI?(YES)

 3.MINOR: Getting input for triggering the record video option.

3.Adding if time permits:
 + Converting the tensorflow model into tflite model and to deploy it in rpi (edge device)
  + using picamera for image capture
  + Sound result produced should be fed into a separate audio output
  + Or bluetooth module of output voice?

4.MINOR:(FIXED)
Tf file to tflite file conversion error:
1. From the official video:
tensorflow.contrib was not importable even after installing tflite,tf,tensorflow_qnd..
therefore
#converter =lite.TFliteConverter.from_keras_model(model)
was used but
module 'tensorflow._api.v2.lite' has no attribute 'TFliteConverter',
AttributeError: 'str' object has no attribute 'call' tflite converter

IMP:
converter.experimental_new_converter = True
WARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. 
The old converter is deprecated.

#converter = lite.TFLiteConverter.from_saved_model("C:\\Users\\Biancaa. R\\lumin_eye\\model1.h5")
or
#converter = lite.TFLiteConverter.from_saved_model("C:\\Users\\Biancaa. R\\lumin_eye")
says model not found at the specified location

//Incase this is not given it doesnt work even when providing the name of hdf5 file:
model=keras.models.load_model("model1.h5")

//if not providing:
#converter.target_spec.supported_ops = [lite.OpsSet.TFLITE_BUILTINS, lite.OpsSet.SELECT_TF_OPS]

 error: 'tf.TensorListReserve' op requires element_shape to be static during TF Lite transformation pass
<unknown>:0: note: loc(fused["StatefulPartitionedCall:", "StatefulPartitionedCall"]): called from
d:\Anaconda\Lib\site-packages\tensorflow\python\saved_model\save.py:1313:0: error: failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(fused["StatefulPartitionedCall:", "StatefulPartitionedCall"]): called from
<unknown>:0: error: Lowering tensor list ops is failed. Please consider using Select TF ops and disabling `_experimental_lower_tensor_list_ops` flag in the TFLite converter object. For example, converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, 
tf.lite.OpsSet.SELECT_TF_OPS]\n converter._experimental_lower_tensor_list_ops = False

5.MINOR (fixed): the indices for iterations should be integers not string
//load the features from the .pkl file not from txt

6.MAJOR(HUGE):<FIXED>
The desired output was not provided by the caption generator
eg. <start> Two dogs playing with a ball end end end end end...
reasons:
#usage of wrong start stop delimiters
# each time the program ran new delimiters were added

(it was taken care of)

Incase the correct delimiters were used..
ex dogs dogs dogs dogs dogs....
(
  could be because of we should rerun the epoches after the changes
  <start>,<stop> special characters are getting removed as well
)

FIX:
Therefore the delimiters are changed to startseq and endseq

BUT:
A lot of "other" is included and the sentences are not gramatically right
The caption describing the same object/person is getting repeated as and,and..
Accuracy is trash

MAJOR:(FIXED)
The text output is produced using pyttsx3 
but the pace seems fast and if time.sleep() is used the video lags so much ...
Threfore processing should be done but not all displayed?

HOW:
Showing the op of only even iterations as audio

7.Accuracy of the model (image recognition ) is low but can be improved by adding more datasets and training the model in a better way so that 
=The sentences are consize
-The accuracy is more
-IMPORTANT THINGS are focused on...
