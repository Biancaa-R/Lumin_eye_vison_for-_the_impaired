{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWXWtICJgd9q",
        "outputId": "7e64e951-b944-4457-91a8-3d50816fad0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/97.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2023.11.17)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=57798f4676ae5bf5aaa3f2a5ffdacd53e323408412dc687ba43dd33ac467b1d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7528cb49bc60c49969227e100d1cd1631f5b2a2eb276248e10393fe60564e5ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-23.12.11 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "9E2cp4nUhiHf",
        "outputId": "0586dbf8-5698-455f-cbc6-2f05c5c351f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-tswAXaivMU",
        "outputId": "2d27cc41-a488-4966-b1b6-77b229ca20e2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TbgdFtfgcS0",
        "outputId": "d7fcf8c5-287c-415d-b842-b195deda8954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The differential amplifier is used to amplify the difference between two input signals, while rejecting any common-mode signal that is present in both input signals.The above circuit shown is dual input balanced output since two inputs are applied at the two bases of BJT transistors Q1 and Q2 and two outputs are taken from the collectors of the transistors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Create a parser with your text\n",
        "parser = PlaintextParser.from_file(\"da.txt\", Tokenizer(\"english\"))\n",
        "\n",
        "# Create a LexRankSummarizer instance\n",
        "summarizer_lex = LexRankSummarizer()\n",
        "\n",
        "# Summarize using LexRank\n",
        "summary = summarizer_lex(parser.document, 2)\n",
        "\n",
        "# Convert the summary to a string\n",
        "lex_summary = \"\"\n",
        "for sentence in summary:\n",
        "    lex_summary += str(sentence)\n",
        "\n",
        "# Print the LexRank summary\n",
        "print(lex_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Your input text\n",
        "your_text = \"\"\"\n",
        "This is your input text. Replace this with the actual text you want to summarize.\n",
        "You can include multiple sentences and paragraphs in this string.\n",
        "\"\"\"\n",
        "\n",
        "# Create a parser with your text\n",
        "parser = PlaintextParser.from_string(your_text, Tokenizer(\"english\"))\n",
        "\n",
        "# Create a LexRankSummarizer instance\n",
        "summarizer_lex = LexRankSummarizer()\n",
        "\n",
        "# Summarize using LexRank\n",
        "summary = summarizer_lex(parser.document, 2)\n",
        "\n",
        "# Convert the summary to a string\n",
        "lex_summary = \"\"\n",
        "for sentence in summary:\n",
        "    lex_summary += str(sentence)\n",
        "\n",
        "# Print the LexRank summary\n",
        "print(lex_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCs5HgsGjFNM",
        "outputId": "5e04c5e4-99d8-491c-f613-60d1ce2f3ecb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your input text.Replace this with the actual text you want to summarize.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''parser.document is an attribute of the parser object, and it represents the processed or parsed version of the input text. In this case, the input text is provided as a string (your_text), and the PlaintextParser is used to convert it into a document that can be processed by the LexRank summarizer.'''\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Your input text\n",
        "your_text = \"\"\"\n",
        "Schoolbag in hand, she leaves home in the early morning.\n",
        "Waving goodbye with an absent-minded smile.\n",
        "I watch her go with a surge of that well-known sadness\n",
        "And I have to sit down for a while.\n",
        "The feeling that I'm losing her forever\n",
        "And without really entering her world.\n",
        "I'm glad whenever I can share her laughter.\n",
        "That funny little girl.\n",
        "Slipping through my fingers all the time.\n",
        "I try to capture every minute.\n",
        "The feeling in it.\n",
        "Slipping through my fingers all the time.\n",
        "Do I really see what's in her mind.\n",
        "Each time I think I'm close to knowing.\n",
        "She keeps on growing.\n",
        "Slipping through my fingers all the time.\n",
        "Sleep in our eyes, her and me at the breakfast table.\n",
        "Barely awake, I let precious time go by.\n",
        "Then when she's gone, there's that odd melancholy feeling\n",
        "And a sense of guilt I can't deny.\n",
        "What happened to the wonderful adventures\n",
        "The places I had planned for us to go.\n",
        "(Slipping through my fingers all the time)\n",
        "Well, some of that we did but most we didn't\n",
        "And why, I just don't know.\n",
        "Slipping through my fingers all the time.\n",
        "I try to capture every minute.\n",
        "The feeling in it.\n",
        "Slipping through my fingers all the time.\n",
        "Do I really see what's in her mind.\n",
        "Each time I think I'm close to knowing.\n",
        "She keeps on growing.\n",
        "Slipping through my fingers all the time.\n",
        "Sometimes I wish that I could freeze the picture\n",
        "And save it from the funny tricks of time.\n",
        "Slipping through my fingers.\n",
        "Slipping through my fingers all the time.\n",
        "Schoolbag in hand she leaves home in the early morning.\n",
        "Waving goodbye with an absent-minded smile.\n",
        "\"\"\"\n",
        "\n",
        "# Create a parser with your text\n",
        "parser = PlaintextParser.from_string(your_text, Tokenizer(\"english\"))\n",
        "\n",
        "# Create a LexRankSummarizer instance\n",
        "summarizer_lex = LexRankSummarizer()\n",
        "\n",
        "desired_percentage = 20  # Adjust this value as needed\n",
        "total_sentences = len(parser.document.sentences)\n",
        "desired_sentences = int(total_sentences * (desired_percentage / 100))\n",
        "\n",
        "# Summarize using LexRank with the desired number of sentences\n",
        "summary = summarizer_lex(parser.document, desired_sentences)\n",
        "\n",
        "\n",
        "# Convert the summary to a string\n",
        "lex_summary = \"\"\n",
        "for sentence in summary:\n",
        "    lex_summary += str(sentence)\n",
        "\n",
        "# Print the LexRank summary\n",
        "print(lex_summary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY3RKQzOj2W6",
        "outputId": "ef90ccee-ad9c-4a4c-836f-69c8a674265e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The feeling that I'm losing her forever And without really entering her world.The feeling in it.Do I really see what's in her mind.What happened to the wonderful adventures The places I had planned for us to go.The feeling in it.Do I really see what's in her mind.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Your input text\n",
        "your_text = \"\"\"\n",
        "Schoolbag in hand, she leaves home in the early morning.\n",
        "Waving goodbye with an absent-minded smile.\n",
        "I watch her go with a surge of that well-known sadness\n",
        "And I have to sit down for a while.\n",
        "The feeling that I'm losing her forever\n",
        "And without really entering her world.\n",
        "I'm glad whenever I can share her laughter.\n",
        "That funny little girl.\n",
        "Slipping through my fingers all the time.\n",
        "I try to capture every minute.\n",
        "The feeling in it.\n",
        "Slipping through my fingers all the time.\n",
        "Do I really see what's in her mind.\n",
        "Each time I think I'm close to knowing.\n",
        "She keeps on growing.\n",
        "Slipping through my fingers all the time.\n",
        "Sleep in our eyes, her and me at the breakfast table.\n",
        "Barely awake, I let precious time go by.\n",
        "Then when she's gone, there's that odd melancholy feeling\n",
        "And a sense of guilt I can't deny.\n",
        "What happened to the wonderful adventures\n",
        "The places I had planned for us to go.\n",
        "(Slipping through my fingers all the time)\n",
        "Well, some of that we did but most we didn't\n",
        "And why, I just don't know.\n",
        "Slipping through my fingers all the time.\n",
        "I try to capture every minute.\n",
        "The feeling in it.\n",
        "Slipping through my fingers all the time.\n",
        "Do I really see what's in her mind.\n",
        "Each time I think I'm close to knowing.\n",
        "She keeps on growing.\n",
        "Slipping through my fingers all the time.\n",
        "Sometimes I wish that I could freeze the picture\n",
        "And save it from the funny tricks of time.\n",
        "Slipping through my fingers.\n",
        "Slipping through my fingers all the time.\n",
        "Schoolbag in hand she leaves home in the early morning.\n",
        "Waving goodbye with an absent-minded smile.\n",
        "\"\"\"\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Create a parser with your text and specify the NLTK tokenizer\n",
        "parser = PlaintextParser.from_string(your_text, Tokenizer(\"english\"))\n",
        "\n",
        "# Create a LexRankSummarizer instance\n",
        "summarizer_lex = LexRankSummarizer()\n",
        "\n",
        "desired_percentage = 70  # Adjust this value as needed\n",
        "total_sentences = len(parser.document.sentences)\n",
        "desired_sentences = int(total_sentences * (desired_percentage / 100))\n",
        "\n",
        "# Summarize using LexRank with the desired number of sentences\n",
        "summary = summarizer_lex(parser.document, desired_sentences)\n",
        "\n",
        "# Convert the summary to a string\n",
        "lex_summary = \"\"\n",
        "for sentence in summary:\n",
        "    lex_summary += str(sentence)\n",
        "\n",
        "# Print the LexRank summary\n",
        "print(lex_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t83oeuvaks-w",
        "outputId": "44f2902d-2637-4abb-a9bf-06ef03da69c5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waving goodbye with an absent-minded smile.The feeling that I'm losing her forever And without really entering her world.Slipping through my fingers all the time.I try to capture every minute.The feeling in it.Slipping through my fingers all the time.Do I really see what's in her mind.She keeps on growing.Slipping through my fingers all the time.Barely awake, I let precious time go by.What happened to the wonderful adventures The places I had planned for us to go.(Slipping through my fingers all the time) Well, some of that we did but most we didn't And why, I just don't know.Slipping through my fingers all the time.The feeling in it.Slipping through my fingers all the time.Do I really see what's in her mind.She keeps on growing.Slipping through my fingers all the time.Sometimes I wish that I could freeze the picture And save it from the funny tricks of time.Slipping through my fingers.Slipping through my fingers all the time.Waving goodbye with an absent-minded smile.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}